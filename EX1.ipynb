{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the Most popular places. Next, we want you to collect the URL associated with each site in the list from this list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to the place's URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first page have a different url we have saved the url in the variable urlP.\\\n",
    "Then the other 399 have a common url, the only different is the number of the page, so we have splitted the url in two parts,\\\n",
    "so every time we want to access a different url we can simply concatenate these two with the number of the page in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlP = \"https://www.atlasobscura.com/places?sort=likes_count\"\n",
    "url1=\"https://www.atlasobscura.com/places?page=\"\n",
    "url2=\"&sort=likes_count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_link(url1,url2):\n",
    "     with open(\"Places/Address.txt\",\"w\") as f:\n",
    "        for i in range(1,401):\n",
    "            if i==1:\n",
    "                result = requests.get(urlP)\n",
    "                soup = bs4.BeautifulSoup(result.text,\"lxml\")\n",
    "                for item in soup.find_all('a',{'class':'content-card content-card-place'}):\n",
    "                    f.write(\"https://www.atlasobscura.com\"+item[\"href\"]+\"\\n\")\n",
    "            else:\n",
    "                i=str(i)\n",
    "                url = url1+i+url2\n",
    "                result = requests.get(url)\n",
    "                soup = bs4.BeautifulSoup(result.text,\"lxml\")\n",
    "                for item in soup.find_all('a',{'class':'content-card content-card-place'}):\n",
    "                    f.write(\"https://www.atlasobscura.com\"+item[\"href\"]+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_single_link(url1,url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl places\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "Download the HTML corresponding to each of the collected URLs.\n",
    "After you collect a single page, immediately save its HTML in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "Organize the entire set of downloaded HTML pages into folders. Each folder will contain the HTML of the places on page 1, page 2, ... of the list of locations.\n",
    "Tip: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Places/Address.txt', 'r') as f:\n",
    "    urls_places = f.readlines()\n",
    "\n",
    "for index, url in enumerate(urls_places):                                                   #variable index is useful to take in account the ranking of the place\n",
    "    page_number = index//18                                                                 #with this variable we are able to allocate the html file in the write folder\n",
    "    r = requests.get(url[:-1])                                                              \n",
    "    filename=f'Page_{page_number+1}/{index+1}_{url[36:-1].replace(\"-\",\" \")}.html'           #url[36:1] since from position 36 starts the name of the place for every url\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)                                   #we have to say that this direction exists otherwise we will be have an error\n",
    "    with open(filename, 'w',encoding=\"utf-8\") as file:\n",
    "        file.write(r.text)\n",
    "        file.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28ea75b7d9dd5b325cb9a751dc34da71a10f8944bac9b4ff454d9595cb15b82f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
