{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of places\n",
    "We start with the list of places to include in your corpus of documents. In particular, we focus on the Most popular places. Next, we want you to collect the URL associated with each site in the list from this list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 18 places, so that you will end up with 7200 unique place URLs).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to the place's URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first page have a different url we have saved the url in the variable urlP.\\\n",
    "Then the other 399 have a common url, the only different is the number of the page, so we have splitted the url in two parts,\\\n",
    "so every time we want to access a different url we can simply concatenate these two with the number of the page in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlP = \"https://www.atlasobscura.com/places?sort=likes_count\"\n",
    "url1=\"https://www.atlasobscura.com/places?page=\"\n",
    "url2=\"&sort=likes_count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_link(url1,url2):\n",
    "     with open(\"Places/Address.txt\",\"w\") as f:\n",
    "        for i in range(1,401):\n",
    "            if i==1:\n",
    "                result = requests.get(urlP)\n",
    "                soup = bs4.BeautifulSoup(result.text,\"lxml\")\n",
    "                for item in soup.find_all('a',{'class':'content-card content-card-place'}):\n",
    "                    f.write(\"https://www.atlasobscura.com\"+item[\"href\"]+\"\\n\")\n",
    "            else:\n",
    "                i=str(i)\n",
    "                url = url1+i+url2\n",
    "                result = requests.get(url)\n",
    "                soup = bs4.BeautifulSoup(result.text,\"lxml\")\n",
    "                for item in soup.find_all('a',{'class':'content-card content-card-place'}):\n",
    "                    f.write(\"https://www.atlasobscura.com\"+item[\"href\"]+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_single_link(url1,url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl places\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "Download the HTML corresponding to each of the collected URLs.\n",
    "After you collect a single page, immediately save its HTML in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "Organize the entire set of downloaded HTML pages into folders. Each folder will contain the HTML of the places on page 1, page 2, ... of the list of locations.\n",
    "Tip: Due to a large number of pages you should download, you can use some methods that can help you shorten the time it takes. If you employed a particular process or approach, kindly describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Places/Address.txt', 'r') as f:\n",
    "    urls_places = f.readlines()\n",
    "\n",
    "for index, url in enumerate(urls_places):                                                  #variable index is useful to take in account the ranking of the place\n",
    "    page_number = index//18                                                                 #with this variable we are able to allocate the html file in the write folder\n",
    "    if index>=0 and index<=8:\n",
    "        filename=f'Page_{page_number+1}/0{index+1}_{url[36:-1].replace(\"-\",\" \")}.html'      #added this part of the code to have all the files ordered\n",
    "    else:\n",
    "        filename=f'Page_{page_number+1}/{index+1}_{url[36:-1].replace(\"-\",\" \")}.html'       #url[36:1] since from position 36 starts the name of the place for every url\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)                                   #we have to say that this direction exists otherwise we will be have an error\n",
    "    try:\n",
    "        if (os.path.getsize(filename) < 2000):\n",
    "            with open(filename, 'w',encoding=\"utf-8\") as file:\n",
    "                r = requests.get(url[:-1])\n",
    "                if r.status_code==200:\n",
    "                    file.write(r.text)\n",
    "                    print(\"Downloaded place \"+str(index+1)+\", Page \"+str(page_number+1))\n",
    "        else:\n",
    "            print(\"done\")\n",
    "    except Exception as e:\n",
    "        print(\"Error occured! \"+ str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages\n",
    "At this point, you should have all the HTML documents about the places of interest, and you can start to extract the places' information. The list of the information we desire for each place and their format is as follows:\n",
    "\n",
    "__Place Name__ (to save as __placeName__): String.\\\n",
    "__Place Tags__ (to save as __placeTags__): List of Strings.\\\n",
    "\\# of people who have been there (to save as __numPeopleVisited__): Integer.\\\n",
    "\\# of people who want to visit the place(to save as __numPeopleWant__): Integer.\\\n",
    "Description (to save as __placeDesc__): String. Everything from under the first image up to \"know before you go\" (orange frame on the example image).\\\n",
    "Short Description (to save as __placeShortDesc____): String. Everything from the title and location up to the image (blue frame on the example image).\\\n",
    "Nearby Places (to save as __placeNearby__): Extract the names of all nearby places, but only keep unique values: List of Strings.\\\n",
    "Address of the place(to save as __placeAddress__): String.\\\n",
    "Altitude and Longitude of the place's location(to save as __placeAlt__ and __placeLong__): Integers\\\n",
    "The username of the post editors (to save as __placeEditors__): List of Strings.\\\n",
    "Post publishing date (to save as __placePubDate__): datetime.\\\n",
    "The names of the lists that the place was included in (to save as __placeRelatedLists__): List of Strings.\\\n",
    "The names of the related places (to save as __placeRelatedPlaces__): List of Strings.\\\n",
    "The URL of the page of the place (to save as __placeURL__):String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le prime 7 righe servono per iterare su tutti i folder, che sono 400, quindi cambiare alla fine in range(1,401)\n",
    "with open('Places/Address.txt', 'r') as f:\n",
    "    urls_places = f.readlines()\n",
    "\n",
    "for i in range(1,401):\n",
    "    page = str(i)\n",
    "    directory = f'Page_{page}'\n",
    "# iterate over files in\n",
    "# that directory\n",
    "    for index,filename in enumerate(os.listdir(directory)):\n",
    "        with open(f\"{directory}/{filename}\", 'r',encoding=\"utf-8\") as f:\n",
    "            soup = bs4.BeautifulSoup(f.read(), 'lxml')\n",
    "            urlPlace = urls_places[((i-1)*18)+(index)]\n",
    "            indice=str(((i-1)*18)+(index)+1)\n",
    "            data = extract_single_place(soup)\n",
    "            data['urlPlace']=urlPlace\n",
    "            file_name = f\"place_{indice}.tsv\"\n",
    "            create_csv(data,file_name)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_place(soup):\n",
    "    placeName = str(soup.find_all('h1',{'class': 'DDPage__header-title'})[0].text)\n",
    "    placeTags = [x.text.replace(\"\\n\",\"\") for x in soup.find_all('a',{'class': 'itemTags__link js-item-tags-link'})]\n",
    "    numPeopleVisited = int(soup.find_all('div',{'class': 'title-md item-action-count'})[0].text)\n",
    "    numPeopleWant = int(soup.find_all('div',{'class': 'title-md item-action-count'})[1].text)\n",
    "    placeDesc = \"\".join([x.text.replace(\"\\xa0\",\"\") for x in soup.find_all('div',{'class':'DDP__body-copy'})])\n",
    "    placeShortDesc = str(soup.find_all('h3',{'class': 'DDPage__header-dek'})[0].contents[0])\n",
    "    placeNearby = [x.text for x in soup.find_all('div',{'class':'DDPageSiderailRecirc__item-title'})]\n",
    "    placeAddress = find_address(soup)\n",
    "    placeAlt,placeLong = AltLong(soup)\n",
    "    placeEditors = [x.text.replace(\"\\n\",\"\") for x in soup.find_all(\"a\",{'class':'DDPContributorsList__contributor'})]\n",
    "    placePubDate = tempo(soup)\n",
    "    placeRelatedLists,placeRelatedPlaces = RelatedPlace(soup)\n",
    "    \n",
    "    return {'placeName':placeName,\n",
    "            'placeTags':placeTags,\n",
    "            'numPeopleVisited':numPeopleVisited,\n",
    "            'numPeopleWant':numPeopleWant,\n",
    "            'placeDesc':placeDesc,\n",
    "            'placeShortDesc':placeShortDesc,\n",
    "            'placeNearby':placeNearby,\n",
    "            'placeAddress':placeAddress,\n",
    "            'placeAlt':placeAlt,\n",
    "            'placeLong':placeLong,\n",
    "            'placeEditors':placeEditors,\n",
    "            'placePubDate':placePubDate,\n",
    "            'placeRelatedLists':placeRelatedLists,\n",
    "            'placeRelatedPlaces':placeRelatedPlaces}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempo(soup):\n",
    "    if len(soup.find_all('div',{'class':'DDPContributor__name'}))>0:\n",
    "        placePubDate = datetime.strptime(soup.find_all('div',{'class':'DDPContributor__name'})[0].text.replace(\",\",\"\"),'%B %d %Y')\n",
    "    else:\n",
    "        placePubDate = np.datetime64(\"NaT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(data,file_name):\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.transpose()\n",
    "    df.to_csv(f'{file_name}',index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RelatedPlace(soup):\n",
    "    RelaPlace = soup.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})\n",
    "    placeRelatedLists = []\n",
    "    placeRelatedPlaces =[]\n",
    "    app=0\n",
    "    for a in soup.find_all('div',{'class':'CardRecircSection__title'}):\n",
    "        if 'Appears in' in a.text:\n",
    "            app = int(a.text[11])\n",
    "    for place in RelaPlace[-app:]:\n",
    "        placeRelatedLists.append(place.span.text)\n",
    "    for place in RelaPlace[-(app+4):-app]:\n",
    "        placeRelatedPlaces.append(place.span.text)\n",
    "    return placeRelatedLists,placeRelatedPlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AltLong(soup):\n",
    "    AltLong = soup.find_all('div',{'class' : 'DDPageSiderail__coordinates js-copy-coordinates'})[0].contents[2].replace(\"\\n\",\"\").replace(\" \",\"\").split(\",\")\n",
    "    placeAlt = float(AltLong[0])\n",
    "    placeLong = float(AltLong[1])\n",
    "    return placeAlt,placeLong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_address(soup):\n",
    "    address = soup.find_all('address',{'class':'DDPageSiderail__address'})\n",
    "    placeAddress =\"\"\n",
    "    for add in address[0].div.contents:\n",
    "        if type(add)==bs4.element.NavigableString:\n",
    "            placeAddress = placeAddress+\" \"+add\n",
    "    return placeAddress\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2875e863656f51b086e8e28d9d22dbee1e08a4c40db222f2cb26ce4a6f7eef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
