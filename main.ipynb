{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algorithmic Methods of Data Mining - Fall 2022**\n",
    "\n",
    "#### Group Members: Yoanna Efimova, Angelo Mandara, Cem Sirin\n",
    "<yoanna.efimova@gmail.com>, <mandara.2077139@studenti.uniroma1.it>, <sirincem1@gmail.com>\n",
    "\n",
    "## **Homework 3: Places of the World**\n",
    "\n",
    "**We structured the notebook such that there is a section and subsection for each question and subquestion. The outline is as follows:**\n",
    "* **Question 1: Data Collection**\n",
    "    * **1.1:** *Getting the list of places*\n",
    "    * **1.2:** *Crawling pages*\n",
    "    * **1.3:** *Parsing pages*\n",
    "* **Question 2: Search Engine**\n",
    "    * **2.1:** *Conjunctive queries*\n",
    "        * **2.1.1:** *Creating an index*\n",
    "        * **2.1.2:** *Querying the index*\n",
    "    * **2.2:** *Conjunctive queries and Ranking Scores*\n",
    "        * **2.2.1:** *Creating an inverted index*\n",
    "        * **2.2.2:** *Querying the inverted index*\n",
    "* **Question 3: Defining a score**\n",
    "* **Question 4: Visualization**\n",
    "* **Question 5: Complex search engines**\n",
    "* **Question 6: Command line interface**\n",
    "* **Question 7: Theoretical Aspects**\n",
    "\n",
    "**Packages that are used troughout the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Organize the libraries\n",
    "\n",
    "# Library for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Library for tracking progress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "# Libraries to scrape the web\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Library to save data\n",
    "import json\n",
    "import pickle\n",
    "# Library to work with dates\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from requests_html import AsyncHTMLSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Data Collection\n",
    "\n",
    "We start by importing the necessary packages and defining the contstants and functions that will be used throughout this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_URL = 'https://www.atlasobscura.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Getting the list of places\n",
    "\n",
    "Our task is to get the list of places in the top 400 pages sorted by popularity. The URLs of those pages follow the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top first 400 pages of Atlas Obscura\n",
    "page_urls = [(f'{BASE_URL}/places?page={i}&sort=likes_count') for i in range(1, 401)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "\n",
    "def save_html(url, path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "\n",
    "    r = s.get(url)\n",
    "    while r.status_code != 200:\n",
    "        r = s.get(url)\n",
    "        time.sleep(30)\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the URLs of the top 400 pages and save their html content in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 106017.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, url in tqdm(enumerate(page_urls), total=len(page_urls)):\n",
    "    path = f'data/pages/page_{i}.html'\n",
    "    save_html(url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we parse over the html content of each page and extract the list of places. We save the URLs of the places in a text file as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:11<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Grabbing the urls of all the places\n",
    "place_urls = []\n",
    "for i in tqdm(range(400)):\n",
    "\n",
    "    with open(f'data/pages/page_{i}.html', 'r') as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_urls.extend([a['href'] for a in soup.find_all('a', class_='content-card content-card-place')])\n",
    "\n",
    "# Save the list of hrefs as text file\n",
    "with open('data/misc/place_urls.txt', 'w') as f:\n",
    "    for url in place_urls:\n",
    "        f.write(f'{url}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all variables that are no longer needed\n",
    "del html, soup, f, i, url, page_urls, place_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of place urls\n",
    "with open('data/misc/place_urls.txt', 'r') as f:\n",
    "    place_urls = [url.strip() for url in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7200/7200 [00:00<00:00, 112391.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for url in tqdm(place_urls):\n",
    "    save_html(BASE_URL + url, f'data{url}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse and extract data from the htmls\n",
    "def parse_place(html, url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    placeName = soup.find('h1', class_='DDPage__header-title').text.strip()\n",
    "    placeTags = [x.text.strip() for x in soup.find('div', class_='item-tags').find_all('a')] if soup.find('div', class_='item-tags') else None\n",
    "    numPeopleVisited = int(soup.find_all('div', class_='title-md item-action-count')[0].text.strip())\n",
    "    numPeopleWant = int(soup.find_all('div', class_='title-md item-action-count')[1].text.strip())\n",
    "    placeDesc = soup.find('div', id='place-body').text.strip()\n",
    "    placeShortDesc = soup.find('h3', class_='DDPage__header-dek').text.strip()\n",
    "    placeNearby = [x['href'].strip() for x in soup.find('div', class_='DDPageSiderailRecirc').find_all('a')]\n",
    "    placeAddress = '; '.join([x.strip() for x in soup.find('address').find('div').contents if isinstance(x, str)])\n",
    "    placeLat = float(soup.find('div', class_='DDPageSiderail__coordinates js-copy-coordinates')['data-coordinates'].split(',')[0])\n",
    "    placeLong = float(soup.find('div', class_='DDPageSiderail__coordinates js-copy-coordinates')['data-coordinates'].split(',')[1])\n",
    "    placeEditors = soup.find('a', class_='DDPContributorsList__contributor')['href'] if soup.find('a', class_='DDPContributorsList__contributor') else None\n",
    "    placePubDate = datetime.strptime(soup.find('div', class_='DDPContributor__name').text.strip(), '%B %d, %Y') if soup.find('div', class_='DDPContributor__name') else None\n",
    "    placeRelatedLists  = [x['href'] for x in soup.find('div', attrs={'data-gtm-template': 'DDP Footer Recirc Lists'}).find_all('a')] if soup.find('div', attrs={'data-gtm-template': 'DDP Footer Recirc Lists'}) else None\n",
    "    placeRelatedPlaces = [x['href'] for x in soup.find('div', attrs={'data-gtm-template': 'DDP Footer Recirc Related'}).find_all('a')] if soup.find('div', attrs={'data-gtm-template': 'DDP Footer Recirc Related'}) else None\n",
    "    placeURL = '/places/' + url\n",
    "    return {\n",
    "        'placeName': placeName,\n",
    "        'placeTags': placeTags,\n",
    "        'numPeopleVisited': numPeopleVisited,\n",
    "        'numPeopleWant': numPeopleWant,\n",
    "        'placeDesc': placeDesc,\n",
    "        'placeShortDesc': placeShortDesc,\n",
    "        'placeNearby': placeNearby,\n",
    "        'placeAddress': placeAddress,\n",
    "        'placeLat': placeLat,\n",
    "        'placeLong': placeLong,\n",
    "        'placeEditors': placeEditors,\n",
    "        'placePubDate': placePubDate,\n",
    "        'placeRelatedLists': placeRelatedLists,\n",
    "        'placeRelatedPlaces': placeRelatedPlaces,\n",
    "        'placeURL': placeURL\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all the html files in the data/places\n",
    "place_data = []\n",
    "for i, file in tqdm(enumerate(os.listdir('data/places'))):\n",
    "    with open(f'data/places/{file}', 'r') as f:\n",
    "        html = f.read()\n",
    "        d = parse_place(html, file.replace('.html', ''))\n",
    "\n",
    "        # Write the values to a tsv file\n",
    "        with open(f'data/parsed_places/place_{i}.tsv', 'w') as f:\n",
    "            writer = csv.writer(f, delimiter='\\t')\n",
    "            writer.writerow(d.values())\n",
    "\n",
    "        place_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a dataframe\n",
    "df = pd.DataFrame(place_data)\n",
    "\n",
    "# Save the dataframe as pickle\n",
    "df.to_pickle('places.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all variables that are no longer needed\n",
    "del place_data, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords # nltk.download('stopwords')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = set(stopwords.words('english')) | set(STOP_WORDS) ## Cem: Here i comine to sets of stopwords to get a bigger list\n",
    "\n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Supress FutureWarning\n",
    "import warnings; warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_column(text_column: pd.Series) -> pd.Series:\n",
    "    # Step 1: Lowercase\n",
    "    text_column = text_column.str.lower()\n",
    "    # Step 2.1: Remove all spaces, i.e., \\n\n",
    "    text_column = text_column.str.replace('\\n', ' ')\n",
    "    # Step 2.2: Remove all punctuation\n",
    "    text_column = text_column.str.replace('[^\\w\\s]', '')\n",
    "    # Step 3: Remove stopwords\n",
    "    text_column = text_column.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    # Lemmatize\n",
    "    text_column = text_column.apply(lambda x: ' '.join([wnl.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    return text_column\n",
    "\n",
    "def preprocess_str(text: str) -> str:\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    # Step 2.1: Remove all spaces, i.e., \\n\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Step 2.2: Remove all punctuation\n",
    "    text = text.replace('[^\\w\\s]', '')\n",
    "    # Step 3: Remove stopwords\n",
    "    text = ([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatize\n",
    "    text = ([wnl.lemmatize(word, 'v') for word in text])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open places.pkl\n",
    "df = pd.read_pickle('data/misc/places.pkl')\n",
    "# Step 1: Lowercase\n",
    "df['placeDescX'] = preprocess_column(df['placeDesc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 words in the vocabulary: ['zone' 'zoning' 'zoo' 'zoological' 'zoology' 'zoom' 'zooming' 'zuni'\n",
      " 'černý' 'černýs']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count terms in each document\n",
    "vectorizer = CountVectorizer(min_df=5)\n",
    "X = vectorizer.fit_transform(df['placeDescX'])\n",
    "\n",
    "# Save the vocabulary\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print('Last 10 words in the vocabulary:', words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15335/15335 [01:21<00:00, 187.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term ID of the word \"coffee\" is: 3081 and the doc_IDs of the document that contains the word \"coffee\" is: [ 101  127  142  239  376  428  499  551  598  718  789  944  971 1047\n",
      " 1155 1156 1169 1266 1284 1398 1428 1440 1534 1549 1723 1773 1860 1880\n",
      " 1886 1932 2044 2056 2093 2184 2186 2237 2280 2290 2445 2581 2650 2755\n",
      " 2868 2884 3016 3018 3092 3107 3110 3123 3187 3213 3302 3420 3423 3428\n",
      " 3596 3682 3936 3951 4043 4153 4163 4214 4257 4264 4319 4358 4370 4399\n",
      " 4407 4420 4561 4702 4739 4810 4894 5006 5155 5273 5319 5338 5498 5539\n",
      " 5650 5766 5786 5802 5804 5852 5896 6070 6092 6105 6132 6363 6398 6513\n",
      " 6535 6573 6575 6596 6748 6903 6969 7020 7186]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {} # Dictionary that maps words to term_id\n",
    "inv_idx = {} # Dictionary that maps term_id to document_id\n",
    "for i, word in tqdm(enumerate(words), total=len(words)):\n",
    "    vocabulary[word] = i\n",
    "    inv_idx[i] = X[:, i].nonzero()[0]\n",
    "\n",
    "print('The term ID of the word \"coffee\" is:', vocabulary['coffee'], 'and the doc_IDs of the document that contains the word \"coffee\" is:', inv_idx[vocabulary['coffee']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(q):\n",
    "    '''q: query string'''\n",
    "\n",
    "    # Preprocess query\n",
    "    q = preprocess_str(q)\n",
    "    print(q)\n",
    "    # Get term IDs for query terms\n",
    "    idx = [vocabulary[word] for word in q if word in vocabulary]\n",
    "    print(idx)\n",
    "    # Document IDs that contain all query terms\n",
    "    docs = set.intersection(*[set(inv_idx[i]) for i in idx])\n",
    "\n",
    "    return df.iloc[list(docs)][['placeDesc', 'placeName', 'placeURL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'museum', 'yomama']\n",
      "[1068, 9156]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3072</th>\n",
       "      <td>Once only open to academics, Lombroso’s Museum...</td>\n",
       "      <td>Cesare Lombroso's Museum of Criminal Anthropology</td>\n",
       "      <td>/places/cesare-lombrosos-museum-of-criminal-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>It’s easy to work up an appetite as you meande...</td>\n",
       "      <td>Museum of Food and Drink</td>\n",
       "      <td>/places/museum-of-food-and-drink-mofad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>With its rich collection of historic and conte...</td>\n",
       "      <td>Philbrook Museum of Art</td>\n",
       "      <td>/places/philbrook-museum-of-art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>Located in Madison County, Tennessee, this par...</td>\n",
       "      <td>Pinson Mounds State Archeological Park</td>\n",
       "      <td>/places/pinson-mounds-state-archeological-park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>Steve McVoy was always fascinated by TV. In mi...</td>\n",
       "      <td>Early Television Museum</td>\n",
       "      <td>/places/early-television-museum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              placeDesc  \\\n",
       "3072  Once only open to academics, Lombroso’s Museum...   \n",
       "2049  It’s easy to work up an appetite as you meande...   \n",
       "7169  With its rich collection of historic and conte...   \n",
       "6661  Located in Madison County, Tennessee, this par...   \n",
       "6662  Steve McVoy was always fascinated by TV. In mi...   \n",
       "\n",
       "                                              placeName  \\\n",
       "3072  Cesare Lombroso's Museum of Criminal Anthropology   \n",
       "2049                           Museum of Food and Drink   \n",
       "7169                            Philbrook Museum of Art   \n",
       "6661             Pinson Mounds State Archeological Park   \n",
       "6662                            Early Television Museum   \n",
       "\n",
       "                                               placeURL  \n",
       "3072  /places/cesare-lombrosos-museum-of-criminal-an...  \n",
       "2049             /places/museum-of-food-and-drink-mofad  \n",
       "7169                    /places/philbrook-museum-of-art  \n",
       "6661     /places/pinson-mounds-state-archeological-park  \n",
       "6662                    /places/early-television-museum  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query\n",
    "q0 = 'American Museum yomama' # I added 'yomama' to test\n",
    "search(q0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Conjunctive query & ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752948/752948 [00:48<00:00, 15588.92it/s]\n"
     ]
    }
   ],
   "source": [
    "inv_idx_tfidf = {} # Dictionary that maps term_id to document_id as a tuple\n",
    "\n",
    "# create an empty list for each term_id\n",
    "inv_idx_tfidf = {term_id: [] for term_id in vocabulary.values()}\n",
    "\n",
    "for doc_id, term_id in tqdm(zip(*X_tfidf.nonzero()), total=X_tfidf.nnz):\n",
    "    inv_idx_tfidf[term_id].append((doc_id, X_tfidf[doc_id, term_id]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tfidf(q: str, k: int = 10):\n",
    "    '''\n",
    "    q: query string\n",
    "    k: number of results to return\n",
    "    '''\n",
    "\n",
    "    # Preprocess query\n",
    "    q = preprocess_str(q)\n",
    "\n",
    "    # Get term IDs for query terms\n",
    "    term_idx = [vocabulary[word] for word in q if word in vocabulary]\n",
    "\n",
    "    # Documents that contain all query terms (intersection)\n",
    "    docs = set.intersection(*[set([i[0] for i in inv_idx_tfidf[i]]) for i in term_idx])\n",
    "\n",
    "    # Initialize a dictionary that maps document_id to score\n",
    "    scores = {doc_id: 0 for doc_id in docs}\n",
    "    for term_id in term_idx:\n",
    "        for doc_id, score in inv_idx_tfidf[term_id]:\n",
    "            if doc_id in docs:\n",
    "                scores[doc_id] += score\n",
    "\n",
    "    # Spread score values over [0, 1]\n",
    "    scores = {k: (v - min(scores.values())) / (max(scores.values()) - min(scores.values())) \\\n",
    "         for k, v in scores.items()}\n",
    "\n",
    "    # Sort the documents by their tf-idf scores\n",
    "    docs = sorted(scores, key=scores.get, reverse=True)\n",
    "    filtered = df.iloc[docs][['placeDesc', 'placeName', 'placeURL']]\n",
    "    filtered['score'] = [scores[doc_id] for doc_id in docs]\n",
    "\n",
    "    return filtered.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = 'American Museum yomama' # I added 'yomama' to test\n",
    "search_tfidf(q0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Comparing Results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_new_score(q: str, k: int = 10):\n",
    "    '''\n",
    "    q: query string\n",
    "    k: number of results to return\n",
    "    '''\n",
    "\n",
    "    filtered = search_tfidf(q, k=7200)\n",
    "\n",
    "    # Preprocess query\n",
    "    q = preprocess_str(q)\n",
    "\n",
    "    # Processed URL words\n",
    "    filtered['placeURLX'] = filtered['placeURL'].apply(\n",
    "        lambda x: [wnl.lemmatize(y) for y in x.split('/')[-1].split('-')])\n",
    "    \n",
    "    # New score\n",
    "    filtered['new_score'] = filtered['placeURLX'].apply(lambda x: len(set(q) & set(x))) + filtered['score']\n",
    "\n",
    "    # spread new_score values over [0, 1]\n",
    "    filtered['new_score'] = (filtered['new_score'] - min(filtered['new_score'])) / \\\n",
    "        (max(filtered['new_score']) - min(filtered['new_score']))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    filtered.drop(['placeURLX', 'score'], axis=1, inplace=True)\n",
    "\n",
    "    return filtered.sort_values(by='new_score', ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "q0 = 'American Museum' # I added 'yomama' to test\n",
    "search_new_score(q0, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the results of TF-IDF vs. New Score\n",
    "Firstly, let's introduce how we calculate the new score. We define the new score of document $d$ given a query $q$ as\n",
    "\n",
    "$$\n",
    "\\text{new-score}(d, q) = \\text{tf-idf-score}(d, q) + |q \\cap \\text{URL}(d)|,\n",
    "$$\n",
    "\n",
    "where $\\text{tf-idf-score}(d, q)$ is the normalized tf-idf score of document $d$ given query $q$, and $|q \\cap \\text{URL}(d)|$ is the number of words in the query that are also in the URL of the document. The top 5 documents for the query \"American Museum\" are shown below.\n",
    "\n",
    "| Rank | New Score                                   | TF-IDF Score |\n",
    "|------|---------------------------------------------|--------------|\n",
    "| 1    | American Writers Museum\t                 | Siriraj Medical Museum |\n",
    "| 2    | American Banjo Museum                       | Museum of the Weird |\n",
    "| 3    | The American Kennel Club Museum of the Dog  | Harvard Museum of Natural History |\n",
    "| 4    | American Museum of Western Art              | Milwaukee Art Museumy |\n",
    "| 5    | American Museum of Magic                    | Sweet Home Cafe |\n",
    "\n",
    "We can see that the new score gives a better result than the TF-IDF score. The new score gives a higher score to documents that have the query words in the URL, which is a good indicator of the relevance of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Let's map the query results to a map using columns placeLat placeLong\n",
    "def plot_map(q, k=10):\n",
    "\n",
    "    # add other columns from df\n",
    "    filtered = pd.merge(df, search_new_score(q, k=k), how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    fig = go.Figure(go.Scattermapbox(\n",
    "        lat=filtered['placeLat'],\n",
    "        lon=filtered['placeLong'],\n",
    "        mode='markers',\n",
    "        marker_color=filtered['new_score'],\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=15,\n",
    "            opacity=0.5,\n",
    "            colorbar=go.scattermapbox.marker.ColorBar(\n",
    "                title='Score'\n",
    "            ) \n",
    "        ),\n",
    "        text=df['placeName'],\n",
    "        hoverinfo='text',\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        hovermode='closest',\n",
    "        margin=dict(l=0, r=0, t=0, b=0)\n",
    "        )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "q0 = 'American Museum' # I added 'yomama' to test\n",
    "plot_map(q0, k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More complex search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Command line question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read df from pickle\n",
    "df = pd.read_pickle('places.pkl')\n",
    "\n",
    "# Remove all tabs (\\t) and newlines (\\n) from the text\n",
    "df['placeDesc'] = df['placeDesc'].apply(lambda x: x.replace('\\t', ' ').replace('\\n', ' '))\n",
    "df.to_csv('df.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many places in Italy, Spain, France, England and United States are there in our dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for country in Italy Spain France England;\n",
    "do\n",
    "    echo \"The number of places in $country:\"\n",
    "    awk -F '\\t' '$8 ~ /'$country'/{c++} END{print c}' df.tsv\n",
    "done\n",
    "\n",
    "echo \"The number of places in United States:\"\n",
    "awk -F '\\t' '$8 ~ /United States/{c++} END{print c}' df.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The average numer of visitors in places in Italy, Spain, France, England and United States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "countries=(Italy Spain France England)\n",
    "\n",
    "for country in \"${countries[@]}\";\n",
    "do\n",
    "    echo \"The average number visitors for $country\"\n",
    "    awk -F '\\t' '$8 ~ /'$country'/{total += $3; count++} END{print total/count}' df.tsv\n",
    "done\n",
    "\n",
    "echo \"The average number of visitors for United States\"\n",
    "awk -F '\\t' '$8 ~ /United States/{total += $3; count++} END{print total/count}' df.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The number of people who wants to visit the places in Italy, Spain, France, England and United States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "countries=(Italy Spain France England)\n",
    "\n",
    "for country in \"${countries[@]}\";\n",
    "do\n",
    "    echo \"The number of people who want to visit $country\"\n",
    "    awk -F '\\t' '$8 ~ /'$country'/{total += $4; count++} END{print total}' df.tsv\n",
    "done\n",
    "\n",
    "echo \"The number of people who want to visit United States\"\n",
    "awk -F '\\t' '$8 ~ /United States/{total += $4; count++} END{print total}' df.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Theoretical question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2875e863656f51b086e8e28d9d22dbee1e08a4c40db222f2cb26ce4a6f7eef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
